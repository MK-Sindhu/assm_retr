{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "092e5048",
   "metadata": {},
   "source": [
    "### After embeddings are created /project/part_embeddings_struct.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e28e686",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/swapnil/anaconda3/envs/occ310/lib/python3.10/site-packages/torch/__config__.py:9: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:119.)\n",
      "  return torch._C._show_config()\n",
      "/home/swapnil/anaconda3/envs/occ310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[15, 8], edge_index=[2, 28], edge_attr=[28, 1], part_ids=[15], assembly_path='/media/swapnil/3f73cc1a-8f9d-4c19-87af-99b3512ff5b2/MK_S/Automate/assemblies/asm_00000/01984c6ecc641bbea1b098c1_7cf8f9bc95fd3a123d51b6d4_d8e061ba74a1304ea959b3f8_default.json', assembly_id='01984c6ecc641bbea1b098c1_7cf8f9bc95fd3a123d51b6d4_d8e061ba74a1304ea959b3f8_default')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from assembly_graph_pyg import AssemblyGraphDataset\n",
    "\n",
    "part_embeddings = torch.load(\"part_embeddings.pt\", map_location=\"cpu\")\n",
    "\n",
    "ds = AssemblyGraphDataset(\n",
    "    \"assembly_filter_out/assembly_graphs_v1.jsonl\",\n",
    "    part_embeddings=part_embeddings\n",
    ")\n",
    "\n",
    "print(ds[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fb6c6e",
   "metadata": {},
   "source": [
    "### plug embeddings into the AssemblyGraphDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18b4ff15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[15, 8], edge_index=[2, 28], edge_attr=[28, 1], part_ids=[15], assembly_path='/media/swapnil/3f73cc1a-8f9d-4c19-87af-99b3512ff5b2/MK_S/Automate/assemblies/asm_00000/01984c6ecc641bbea1b098c1_7cf8f9bc95fd3a123d51b6d4_d8e061ba74a1304ea959b3f8_default.json', assembly_id='01984c6ecc641bbea1b098c1_7cf8f9bc95fd3a123d51b6d4_d8e061ba74a1304ea959b3f8_default')\n",
      "x dim: 8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from assembly_graph_pyg import AssemblyGraphDataset\n",
    "\n",
    "part_embeddings = torch.load(\"part_embeddings.pt\", map_location=\"cpu\")\n",
    "\n",
    "ds = AssemblyGraphDataset(\n",
    "    \"assembly_filter_out/assembly_graphs_v1.jsonl\",\n",
    "    part_embeddings=part_embeddings\n",
    ")\n",
    "\n",
    "d0 = ds[0]\n",
    "print(d0)\n",
    "print(\"x dim:\", d0.x.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964c4b44",
   "metadata": {},
   "source": [
    "### do all assemblies have embeddings coverage?\n",
    "\n",
    "If any part id is missing in embeddings, dataset loading will throw an error.\n",
    "Run this quick coverage test on the first 200 assemblies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe605fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checked assemblies: 200\n",
      "Missing part embeddings (count): 0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "GRAPHS = Path(\"assembly_filter_out/assembly_graphs_v1.jsonl\")\n",
    "\n",
    "missing_parts = 0\n",
    "checked = 0\n",
    "\n",
    "with GRAPHS.open() as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i >= 200:\n",
    "            break\n",
    "        rec = json.loads(line)\n",
    "        assembly_path = rec[\"assembly_path\"]\n",
    "\n",
    "        assembly = json.load(open(assembly_path))\n",
    "        for p in assembly.get(\"parts\", []):\n",
    "            pid = p.get(\"id\")\n",
    "            if isinstance(pid, str) and pid not in part_embeddings:\n",
    "                missing_parts += 1\n",
    "\n",
    "        checked += 1\n",
    "\n",
    "print(\"Checked assemblies:\", checked)\n",
    "print(\"Missing part embeddings (count):\", missing_parts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab551b4",
   "metadata": {},
   "source": [
    "### forward pass again (now meaningful features)\n",
    "\n",
    "Run the same forward-pass cell, just adjust in_dim=8 automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc226f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch graphs: 8\n",
      "Batch nodes: 20\n",
      "Output shape: torch.Size([8, 32])\n",
      "Example output row: tensor([ 14.8924, -10.6971, -12.8809, -10.0885,  11.0672,   2.8783,   8.8975,\n",
      "         -8.1643,   4.9358,  -8.4923])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "loader = DataLoader(ds, batch_size=8, shuffle=True)\n",
    "\n",
    "class TinyGNN(torch.nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim=64, out_dim=32):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.lin = torch.nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = global_mean_pool(x, batch)\n",
    "        return self.lin(x)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TinyGNN(in_dim=ds[0].x.shape[1]).to(device)\n",
    "\n",
    "batch = next(iter(loader)).to(device)\n",
    "out = model(batch)\n",
    "\n",
    "print(\"Batch graphs:\", batch.num_graphs)\n",
    "print(\"Batch nodes:\", batch.num_nodes)\n",
    "print(\"Output shape:\", out.shape)\n",
    "print(\"Example output row:\", out[0].detach().cpu()[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f872837",
   "metadata": {},
   "source": [
    "### Experiment 1: Do assembly embeddings correlate with assembly complexity?\n",
    "\n",
    "We’ll compute, for a sample of assemblies:\n",
    "\n",
    "num_parts\n",
    "\n",
    "num_edges (undirected and directed)\n",
    "\n",
    "mean and standard deviation of node features\n",
    "\n",
    "graph embedding norm (from the model output)\n",
    "\n",
    "simple correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bbbeb0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   graph_idx  num_nodes  num_edges_undirected  num_edges_directed      x_mean  \\\n",
      "0          0         15                    14                  28  579.688232   \n",
      "1          1          2                     1                   2   12.266267   \n",
      "2          2          9                     8                  16   44.318012   \n",
      "3          3          1                     0                   0  563.009460   \n",
      "4          4          4                     3                   6   88.136536   \n",
      "\n",
      "         x_std     emb_norm   emb_mean     emb_std  \n",
      "0  1723.321899  1026.231812  37.043854  177.591507  \n",
      "1    14.322392    13.313927   0.094850    2.351680  \n",
      "2   138.155579    66.811562   3.557670   11.262161  \n",
      "3  1081.587769   931.203430  -5.437477  164.525238  \n",
      "4   141.435913   132.843216   1.175073   23.454168  \n",
      "\n",
      "=== Correlations (Pearson) ===\n",
      "                      num_nodes  num_edges_undirected    x_mean     x_std  \\\n",
      "num_nodes              1.000000              0.994369  0.000566  0.002286   \n",
      "num_edges_undirected   0.994369              1.000000 -0.000469 -0.000429   \n",
      "x_mean                 0.000566             -0.000469  1.000000  0.972598   \n",
      "x_std                  0.002286             -0.000429  0.972598  1.000000   \n",
      "emb_norm               0.000516             -0.000401  0.997625  0.983220   \n",
      "emb_mean               0.000720             -0.000211  0.997474  0.982786   \n",
      "emb_std                0.000514             -0.000404  0.997625  0.983231   \n",
      "\n",
      "                      emb_norm  emb_mean   emb_std  \n",
      "num_nodes             0.000516  0.000720  0.000514  \n",
      "num_edges_undirected -0.000401 -0.000211 -0.000404  \n",
      "x_mean                0.997625  0.997474  0.997625  \n",
      "x_std                 0.983220  0.982786  0.983231  \n",
      "emb_norm              1.000000  0.999875  1.000000  \n",
      "emb_mean              0.999875  1.000000  0.999870  \n",
      "emb_std               1.000000  0.999870  1.000000  \n",
      "\n",
      "Quick summaries:\n",
      "                             mean           std       min            max\n",
      "num_nodes                5.847000     50.827621  1.000000    1584.000000\n",
      "num_edges_undirected     4.289000     50.521361  0.000000    1582.000000\n",
      "x_mean                1314.853674  12525.390149  5.524045  341543.031250\n",
      "x_std                 3588.547879  32978.209825  4.599179  821744.687500\n",
      "emb_norm              2474.653804  24156.282458  4.130242  654337.937500\n",
      "emb_mean                78.091376    792.722064 -9.302892   21485.986328\n",
      "emb_std                430.113008   4196.076420  0.727612  113658.679688\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# -------- Settings --------\n",
    "N_GRAPHS = 1000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# -------- DataLoader subset --------\n",
    "subset = [ds[i] for i in range(min(N_GRAPHS, len(ds)))]\n",
    "loader = DataLoader(subset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# -------- Model (same as before) --------\n",
    "class TinyGNN(torch.nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim=64, out_dim=32):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.lin = torch.nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = global_mean_pool(x, batch)\n",
    "        return self.lin(x)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TinyGNN(in_dim=ds[0].x.shape[1]).to(device)\n",
    "model.eval()\n",
    "\n",
    "rows = []\n",
    "with torch.no_grad():\n",
    "    idx_base = 0\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        out = model(batch)  # [B, 32]\n",
    "\n",
    "        # Split batch graphs\n",
    "        # batch.ptr gives node pointer per graph; batch.batch gives graph assignment per node\n",
    "        num_graphs = batch.num_graphs\n",
    "\n",
    "        # For each graph in the batch, compute simple stats\n",
    "        for g_idx in range(num_graphs):\n",
    "            # mask nodes belonging to this graph\n",
    "            node_mask = (batch.batch == g_idx)\n",
    "            xg = batch.x[node_mask]  # [Ng, 8]\n",
    "\n",
    "            # edges: edge_index is directed (we doubled edges earlier)\n",
    "            # approximate per-graph directed edges count by filtering edges whose source node is in this graph\n",
    "            src = batch.edge_index[0]\n",
    "            edge_mask = node_mask[src]\n",
    "            directed_edges = int(edge_mask.sum().item())\n",
    "            undirected_edges = directed_edges // 2\n",
    "\n",
    "            emb = out[g_idx]\n",
    "            rows.append({\n",
    "                \"graph_idx\": idx_base + g_idx,\n",
    "                \"num_nodes\": int(xg.shape[0]),\n",
    "                \"num_edges_undirected\": undirected_edges,\n",
    "                \"num_edges_directed\": directed_edges,\n",
    "                \"x_mean\": float(xg.mean().item()),\n",
    "                \"x_std\": float(xg.std(unbiased=False).item()),\n",
    "                \"emb_norm\": float(emb.norm().item()),\n",
    "                \"emb_mean\": float(emb.mean().item()),\n",
    "                \"emb_std\": float(emb.std(unbiased=False).item()),\n",
    "            })\n",
    "\n",
    "        idx_base += num_graphs\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\n=== Correlations (Pearson) ===\")\n",
    "cols = [\"num_nodes\", \"num_edges_undirected\", \"x_mean\", \"x_std\", \"emb_norm\", \"emb_mean\", \"emb_std\"]\n",
    "corr = df[cols].corr(numeric_only=True)\n",
    "print(corr)\n",
    "\n",
    "print(\"\\nQuick summaries:\")\n",
    "print(df[cols].describe().T[[\"mean\",\"std\",\"min\",\"max\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a849417",
   "metadata": {},
   "source": [
    "### Experiment: feature normalization (one cell)\n",
    "Notebook cell: compute normalization stats + wrap dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68f81d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu: tensor([3.0167, 4.4207, 1.6734, 2.4733, 0.2953, 2.7081, 3.6028, 5.3299])\n",
      "sigma: tensor([1.1093e+00, 1.2978e+00, 2.3341e-01, 9.2471e-01, 1.8325e-01, 1.0000e-08,\n",
      "        2.4772e+00, 2.5690e+00])\n",
      "tensor([[ 0.3466,  0.5821,  1.2096,  0.5095, -0.5409,  0.0000,  0.8889,  0.8992],\n",
      "        [ 0.5357,  0.8005,  1.4631,  0.6178, -0.6819,  0.0000,  0.6156,  0.6658],\n",
      "        [ 0.5357,  0.8005,  1.4631,  0.6178, -0.6819,  0.0000,  0.5667,  0.6663]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# Collect feature statistics over a sample (or full dataset)\n",
    "N_STATS = 5000  # set 10920 for full usable set\n",
    "subset = [ds[i] for i in range(min(N_STATS, len(ds)))]\n",
    "\n",
    "# Stack all node features (may be big; this is manageable for 5000)\n",
    "X_all = torch.cat([d.x for d in subset], dim=0).float()\n",
    "\n",
    "# Log transform heavy-tailed dims (all dims here are heavy-tailed in practice)\n",
    "X_log = torch.log1p(torch.clamp(X_all, min=0))  # safe for non-negative stats\n",
    "\n",
    "mu = X_log.mean(dim=0)\n",
    "sigma = X_log.std(dim=0, unbiased=False) + 1e-8\n",
    "\n",
    "print(\"mu:\", mu)\n",
    "print(\"sigma:\", sigma)\n",
    "\n",
    "def normalize_x(x):\n",
    "    x = x.float()\n",
    "    x = torch.log1p(torch.clamp(x, min=0))\n",
    "    return (x - mu) / sigma\n",
    "\n",
    "# Build a normalized view of ds without rewriting files\n",
    "class NormalizedWrapper(torch.utils.data.Dataset):\n",
    "    def __init__(self, base_ds):\n",
    "        self.base = base_ds\n",
    "    def __len__(self):\n",
    "        return len(self.base)\n",
    "    def __getitem__(self, idx):\n",
    "        d = self.base[idx]\n",
    "        d.x = normalize_x(d.x)\n",
    "        return d\n",
    "\n",
    "ds_norm = NormalizedWrapper(ds)\n",
    "\n",
    "print(ds_norm[0].x[:3])  # sanity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9007820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mu: tensor([3.0935, 4.4779, 1.6569, 2.5016, 0.2800, 2.7080, 3.4804,    nan])\n",
      "sigma: tensor([1.1754e+00, 1.3934e+00, 2.6066e-01, 9.9911e-01, 1.9047e-01, 1.0000e-08,\n",
      "        2.5162e+00,        nan])\n",
      "✅ Saved normalization stats to x_norm_stats.pt\n",
      "Before (first node): tensor([2.9000e+01, 1.7600e+02, 6.0690e+00, 1.8000e+01, 2.1675e-01, 1.4000e+01,\n",
      "        3.3086e+02, 2.0788e+03])\n",
      "After  (first node): tensor([ 0.2617,  0.5011,  1.1464,  0.4433, -0.4402, 47.6837,  0.9237,     nan])\n",
      "After stats (mean≈0, std≈1) on sample: nan nan\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from copy import deepcopy\n",
    "\n",
    "# ---------------------------------------\n",
    "# 1) Compute normalization stats (mu/sigma)\n",
    "# ---------------------------------------\n",
    "N_STATS = 10920  # use full usable set; change if you want a smaller sample\n",
    "subset = [ds[i] for i in range(min(N_STATS, len(ds)))]\n",
    "\n",
    "# Stack all node features\n",
    "X_all = torch.cat([d.x for d in subset], dim=0).float()\n",
    "\n",
    "# Log transform (features are non-negative stats; clamp for safety)\n",
    "X_log = torch.log1p(torch.clamp(X_all, min=0))\n",
    "\n",
    "mu = X_log.mean(dim=0)\n",
    "sigma = X_log.std(dim=0, unbiased=False) + 1e-8\n",
    "\n",
    "print(\"mu:\", mu)\n",
    "print(\"sigma:\", sigma)\n",
    "\n",
    "# Save stats for reuse (recommended)\n",
    "torch.save({\"mu\": mu, \"sigma\": sigma}, \"x_norm_stats.pt\")\n",
    "print(\"✅ Saved normalization stats to x_norm_stats.pt\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2) Normalization function\n",
    "# -------------------------------\n",
    "def normalize_x(x, mu, sigma):\n",
    "    x = x.float()\n",
    "    x = torch.log1p(torch.clamp(x, min=0))\n",
    "    return (x - mu) / sigma\n",
    "\n",
    "# ---------------------------------------\n",
    "# 3) Safe dataset wrapper (no in-place mutation)\n",
    "# ---------------------------------------\n",
    "class NormalizedWrapper(torch.utils.data.Dataset):\n",
    "    def __init__(self, base_ds, mu, sigma):\n",
    "        self.base = base_ds\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        d = deepcopy(self.base[idx])  # prevents modifying cached objects\n",
    "        d.x = normalize_x(d.x, self.mu, self.sigma)\n",
    "        return d\n",
    "\n",
    "ds_norm = NormalizedWrapper(ds, mu, sigma)\n",
    "\n",
    "# Sanity check\n",
    "print(\"Before (first node):\", ds[0].x[0])\n",
    "print(\"After  (first node):\", ds_norm[0].x[0])\n",
    "print(\"After stats (mean≈0, std≈1) on sample:\",\n",
    "      ds_norm[0].x.mean().item(), ds_norm[0].x.std(unbiased=False).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aed0754",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "occ310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
